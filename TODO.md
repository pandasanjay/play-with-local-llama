# TODO
- [x] Successfuly steup cuda and nvida driver to work with WSL and Python env setup
- [x] Env setup where I download Lama model and run it localy using `hugging face transform`
  - Q? Using transfomrs I am not getting answer, it is just autocompletes. Same think with Olama
  - Understand transforms and how I can use with Agentic AI systm.
- [X] Run the same model using `Ollaman` As well Llama3.2:1B
- [X] Setup langchain - Buidl a chain, learned about gguf, langchan and evolution and llama.cpp and some new terms llama.cpp
- [ ] Find out what othe alternatives available to langchain and what are prod and cons.
- [ ] Build first ever Multimodal App
- [ ] Spend sometime understand diffrent Model and How I can us it and when.  
- [ ] How to use a hugging face hosted model?


# OPTIONAL EXPLORATION:

- [ ] What is Quantization? How that is works 
- [ ] What is LLM inference? llama.cpp